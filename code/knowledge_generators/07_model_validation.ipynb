{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07 - Model Validation (ASOP 56 Compliant)\n",
                "\n",
                "## Objectives\n",
                "1. Out-of-time holdout validation (train 2012-2017, test 2018-2019)\n",
                "2. Calibration analysis by decile\n",
                "3. ML vs Traditional actuarial comparison\n",
                "4. Stability analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import lightgbm as lgb\n",
                "import matplotlib.pyplot as plt\n",
                "import json\n",
                "from pathlib import Path\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
                "\n",
                "# Paths\n",
                "DATA_PATH = '../data/ilec_cleaned.parquet'\n",
                "MODEL_PATH = '../models/lgbm_mortality_offset_poisson.txt'\n",
                "OUTPUT_DIR = Path('../knowledge_base/methodology')\n",
                "\n",
                "# Features\n",
                "FEATURES = ['Attained_Age', 'Issue_Age', 'Duration', 'Sex', 'Smoker_Status',\n",
                "            'Insurance_Plan', 'Face_Amount_Band', 'Preferred_Class',\n",
                "            'SOA_Post_Lvl_Ind', 'SOA_Antp_Lvl_TP', 'SOA_Guar_Lvl_TP']\n",
                "CAT_FEATURES = ['Sex', 'Smoker_Status', 'Insurance_Plan', 'Face_Amount_Band',\n",
                "                'Preferred_Class', 'SOA_Post_Lvl_Ind', 'SOA_Antp_Lvl_TP', 'SOA_Guar_Lvl_TP']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "print('Loading data...')\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "print(f'Total records: {len(df):,}')\n",
                "print(f'Year range: {df[\"Year\"].min()} - {df[\"Year\"].max()}')\n",
                "\n",
                "# Load model\n",
                "model = lgb.Booster(model_file=MODEL_PATH)\n",
                "print(f'Model loaded: {model.num_trees()} trees')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Out-of-Time Validation\n",
                "\n",
                "Train on 2012-2017, validate on 2018-2019."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split by year\n",
                "train_df = df[df['Year'] <= 2017].copy()\n",
                "test_df = df[df['Year'] >= 2018].copy()\n",
                "\n",
                "print(f'Train: {len(train_df):,} records (2012-2017)')\n",
                "print(f'Test:  {len(test_df):,} records (2018-2019)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode categoricals\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "encoders = {}\n",
                "for col in CAT_FEATURES:\n",
                "    encoders[col] = LabelEncoder()\n",
                "    train_df[col] = encoders[col].fit_transform(train_df[col].astype(str))\n",
                "    test_df[col] = encoders[col].transform(test_df[col].astype(str))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train fresh model on 2012-2017 only\n",
                "X_train = train_df[FEATURES]\n",
                "y_train = train_df['Death_Count']\n",
                "offset_train = np.log(train_df['Policies_Exposed'])\n",
                "\n",
                "X_test = test_df[FEATURES]\n",
                "y_test = test_df['Death_Count']\n",
                "offset_test = np.log(test_df['Policies_Exposed'])\n",
                "\n",
                "# Create datasets\n",
                "train_data = lgb.Dataset(X_train, label=y_train, init_score=offset_train)\n",
                "\n",
                "# Train\n",
                "params = {\n",
                "    'objective': 'poisson',\n",
                "    'metric': 'poisson',\n",
                "    'max_depth': 6,\n",
                "    'learning_rate': 0.1,\n",
                "    'min_child_samples': 100,\n",
                "    'verbosity': -1\n",
                "}\n",
                "\n",
                "oot_model = lgb.train(\n",
                "    params,\n",
                "    train_data,\n",
                "    num_boost_round=100\n",
                ")\n",
                "print('Out-of-time model trained')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on holdout\n",
                "test_df['Expected_Deaths'] = oot_model.predict(X_test) * test_df['Policies_Exposed']\n",
                "\n",
                "# Calculate A/E\n",
                "total_actual = test_df['Death_Count'].sum()\n",
                "total_expected = test_df['Expected_Deaths'].sum()\n",
                "oot_ae = total_actual / total_expected\n",
                "\n",
                "print('=== Out-of-Time Validation (2018-2019) ===')\n",
                "print(f'Actual Deaths:   {total_actual:,.0f}')\n",
                "print(f'Expected Deaths: {total_expected:,.0f}')\n",
                "print(f'A/E Ratio:       {oot_ae:.4f}')\n",
                "print()\n",
                "if 0.95 <= oot_ae <= 1.05:\n",
                "    print('✓ Model is well-calibrated on holdout data')\n",
                "else:\n",
                "    print(f'⚠ Model shows {\"underestimation\" if oot_ae > 1 else \"overestimation\"} on holdout')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A/E by year (holdout)\n",
                "oot_yearly = test_df.groupby('Year').agg({\n",
                "    'Death_Count': 'sum',\n",
                "    'Expected_Deaths': 'sum'\n",
                "}).reset_index()\n",
                "oot_yearly['AE_Ratio'] = oot_yearly['Death_Count'] / oot_yearly['Expected_Deaths']\n",
                "\n",
                "print('=== Holdout A/E by Year ===')\n",
                "print(oot_yearly.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Calibration by Decile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create deciles based on predicted rate\n",
                "test_df['pred_rate'] = test_df['Expected_Deaths'] / test_df['Policies_Exposed']\n",
                "test_df['decile'] = pd.qcut(test_df['pred_rate'], 10, labels=False, duplicates='drop')\n",
                "\n",
                "# A/E by decile\n",
                "calibration = test_df.groupby('decile').agg({\n",
                "    'Death_Count': 'sum',\n",
                "    'Expected_Deaths': 'sum',\n",
                "    'Policies_Exposed': 'sum',\n",
                "    'pred_rate': 'mean'\n",
                "}).reset_index()\n",
                "calibration['actual_rate'] = calibration['Death_Count'] / calibration['Policies_Exposed']\n",
                "calibration['AE_Ratio'] = calibration['Death_Count'] / calibration['Expected_Deaths']\n",
                "\n",
                "print('=== Calibration by Predicted Rate Decile ===')\n",
                "print(calibration[['decile', 'pred_rate', 'actual_rate', 'AE_Ratio']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calibration plot\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "ax.bar(calibration['decile'], calibration['AE_Ratio'], \n",
                "       color='steelblue', alpha=0.7, label='A/E Ratio')\n",
                "ax.axhline(1.0, color='red', linestyle='--', label='Perfect Calibration')\n",
                "ax.axhline(0.95, color='orange', linestyle=':', alpha=0.7)\n",
                "ax.axhline(1.05, color='orange', linestyle=':', alpha=0.7, label='±5% Band')\n",
                "\n",
                "ax.set_xlabel('Predicted Rate Decile (0=Lowest, 9=Highest)')\n",
                "ax.set_ylabel('A/E Ratio')\n",
                "ax.set_title('Model Calibration by Predicted Rate Decile\\n(Out-of-Time: 2018-2019)')\n",
                "ax.legend()\n",
                "ax.set_ylim(0.8, 1.2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../data/plots/validation_calibration_decile.png', dpi=150)\n",
                "plt.show()\n",
                "print('✓ Saved: validation_calibration_decile.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ML vs Traditional Actuarial Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Traditional approach: A/E by Age×Sex\n",
                "# This simulates a basic actuarial table adjustment\n",
                "\n",
                "# Calculate simple age-sex factors from training data\n",
                "age_bins = pd.cut(train_df['Attained_Age'], bins=[0, 30, 50, 65, 80, 120], \n",
                "                  labels=['<30', '30-49', '50-64', '65-79', '80+'])\n",
                "train_df['age_band'] = age_bins\n",
                "\n",
                "# Simple factor table\n",
                "trad_factors = train_df.groupby(['age_band', 'Sex']).agg({\n",
                "    'Death_Count': 'sum',\n",
                "    'Policies_Exposed': 'sum'\n",
                "}).reset_index()\n",
                "trad_factors['rate'] = trad_factors['Death_Count'] / trad_factors['Policies_Exposed']\n",
                "\n",
                "print('=== Traditional Actuarial Factors (Age×Sex) ===')\n",
                "print(trad_factors.pivot(index='age_band', columns='Sex', values='rate').round(6))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply traditional factors to test data\n",
                "test_df['age_band'] = pd.cut(test_df['Attained_Age'], bins=[0, 30, 50, 65, 80, 120],\n",
                "                             labels=['<30', '30-49', '50-64', '65-79', '80+'])\n",
                "test_df = test_df.merge(trad_factors[['age_band', 'Sex', 'rate']].rename(columns={'rate': 'trad_rate'}),\n",
                "                        on=['age_band', 'Sex'], how='left')\n",
                "test_df['trad_expected'] = test_df['trad_rate'] * test_df['Policies_Exposed']\n",
                "\n",
                "# Compare\n",
                "ml_ae = test_df['Death_Count'].sum() / test_df['Expected_Deaths'].sum()\n",
                "trad_ae = test_df['Death_Count'].sum() / test_df['trad_expected'].sum()\n",
                "\n",
                "ml_rmse = np.sqrt(mean_squared_error(test_df['Death_Count'], test_df['Expected_Deaths']))\n",
                "trad_rmse = np.sqrt(mean_squared_error(test_df['Death_Count'], test_df['trad_expected']))\n",
                "\n",
                "print('=== ML vs Traditional Comparison (On Holdout) ===')\n",
                "print(f'{\"Metric\":<20} {\"ML (LightGBM)\":<15} {\"Traditional\":<15}')\n",
                "print(f'{\"A/E Ratio\":<20} {ml_ae:<15.4f} {trad_ae:<15.4f}')\n",
                "print(f'{\"RMSE\":<20} {ml_rmse:<15.4f} {trad_rmse:<15.4f}')\n",
                "print()\n",
                "if abs(ml_ae - 1) < abs(trad_ae - 1):\n",
                "    print('✓ ML model shows better calibration')\n",
                "else:\n",
                "    print('⚠ Traditional approach shows better calibration')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Validation Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save validation results\n",
                "validation_results = {\n",
                "    \"out_of_time_validation\": {\n",
                "        \"train_period\": \"2012-2017\",\n",
                "        \"test_period\": \"2018-2019\",\n",
                "        \"total_actual\": int(total_actual),\n",
                "        \"total_expected\": round(total_expected, 0),\n",
                "        \"ae_ratio\": round(oot_ae, 4),\n",
                "        \"status\": \"calibrated\" if 0.95 <= oot_ae <= 1.05 else \"needs_review\"\n",
                "    },\n",
                "    \"calibration_by_decile\": calibration[['decile', 'AE_Ratio']].to_dict('records'),\n",
                "    \"ml_vs_traditional\": {\n",
                "        \"ml_ae\": round(ml_ae, 4),\n",
                "        \"trad_ae\": round(trad_ae, 4),\n",
                "        \"ml_rmse\": round(ml_rmse, 4),\n",
                "        \"trad_rmse\": round(trad_rmse, 4),\n",
                "        \"ml_better\": abs(ml_ae - 1) < abs(trad_ae - 1)\n",
                "    }\n",
                "}\n",
                "\n",
                "output_path = OUTPUT_DIR / 'validation_results.json'\n",
                "with open(output_path, 'w') as f:\n",
                "    json.dump(validation_results, f, indent=2)\n",
                "    \n",
                "print(f'✓ Saved: {output_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('=== Validation Summary ===')\n",
                "print(f'Out-of-Time A/E: {oot_ae:.4f}')\n",
                "print(f'ML vs Traditional: {\"ML better\" if validation_results[\"ml_vs_traditional\"][\"ml_better\"] else \"Traditional better\"}')\n",
                "print(f'Calibration: All deciles within acceptable range' if all(0.8 < x < 1.2 for x in calibration['AE_Ratio']) else 'Some deciles outside range')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}